{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports for the full reconstruction pipeline \n",
    "\n",
    "\n",
    "# From SIMScope3D Reconstruction\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from skimage.exposure import match_histograms, rescale_intensity\n",
    "from skimage.restoration import calibrate_denoiser, denoise_tv_chambolle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "# From VolPy/Mesmerize \n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import logging\n",
    "import inspect\n",
    "\n",
    "#os.environ[\"MESMERIZE_N_PROCESSES\"] = '40'\n",
    "\n",
    "try:\n",
    "    cv2.setNumThreads(0)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    if __IPYTHON__:\n",
    "        # this is used for debugging purposes only. allows to reload classes\n",
    "        # when changed\n",
    "        get_ipython().magic('load_ext autoreload')\n",
    "        get_ipython().magic('autoreload 2')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.paths import caiman_datadir\n",
    "from caiman.source_extraction.volpy import utils\n",
    "from caiman.source_extraction.volpy.volparams import volparams\n",
    "from caiman.source_extraction.volpy.volpy import VOLPY\n",
    "from caiman.summary_images import local_correlations_movie_offline\n",
    "from caiman.summary_images import mean_image\n",
    "from caiman.utils.utils import download_demo, download_model\n",
    "\n",
    "logging.basicConfig(format=\n",
    "                    \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s]\" \\\n",
    "                    \"[%(process)d] %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# From DeepCAD-RT (Python 3.9.18)\n",
    "from deepcad.train_collection import training_class, testing_class\n",
    "from deepcad.movie_display import display\n",
    "from deepcad.utils import get_first_filename,download_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Seperate raw stack into individual stacks for each phase\n",
    "\n",
    "def seperate_phase(imgs):\n",
    "    p1 = imgs[0::3,:,:]\n",
    "    p2 = imgs[1::3,:,:]\n",
    "    p3 = imgs[2::3,:,:]\n",
    "    return p1, p2, p3\n",
    "\n",
    "def save_phases(filename):\n",
    "    img = tifffile.imread(filename)\n",
    "    p1,p2,p3 = seperate_phase(img)\n",
    "    tifffile.imsave(filename+'_p1.tif', p1)\n",
    "    tifffile.imsave(filename+'_p2.tif', p2)\n",
    "    tifffile.imsave(filename+'_p3.tif', p3)\n",
    "\n",
    "if False:\n",
    "    filename = 'O:\\\\workingdirectory\\\\072623_PVG8\\\\best_sofar\\\\\\SIM900_b4_GOOD.tif'\n",
    "    save_phases(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Motion Correct Each Phase\n",
    "\n",
    "# NoRMCorre implementation (adapted from VolPy)\n",
    "def bulk_MC(filepath,save_name):\n",
    "    print(\"bulk\")\n",
    "    \n",
    "    fnames = os.fsdecode(filepath)\n",
    "    file_dir = os.path.split(fnames)[0]\n",
    "    \n",
    "\n",
    "    fr = 500                                    \n",
    "    ROIs = None                                   \n",
    "    index = None                                  \n",
    "    weights = None                                  \n",
    "                                                    \n",
    "    # Motion correction parameters\n",
    "    pw_rigid = False                              \n",
    "    gSig_filt = (6, 6)                            \n",
    "                                                   \n",
    "    max_shifts = (10, 10)                           \n",
    "    strides = (20, 20)                             \n",
    "    overlaps = (24, 24)                           \n",
    "    max_deviation_rigid = 3                        \n",
    "    border_nan = 'copy'\n",
    "\n",
    "    opts_dict = {\n",
    "        'fnames': fnames,\n",
    "        'fr': fr,\n",
    "        'index': index,\n",
    "        'ROIs': ROIs,\n",
    "        'weights': weights,\n",
    "        'pw_rigid': pw_rigid,\n",
    "        'max_shifts': max_shifts,\n",
    "        'gSig_filt': gSig_filt,\n",
    "        'strides': strides,\n",
    "        'overlaps': overlaps,\n",
    "        'max_deviation_rigid': max_deviation_rigid,\n",
    "        'border_nan': border_nan\n",
    "    }\n",
    "\n",
    "    opts = volparams(params_dict=opts_dict)\n",
    "\n",
    "    if 'dview' in locals():\n",
    "        cm.stop_server(dview=dview)\n",
    "\n",
    "    c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "            backend='local', n_processes=50, single_thread=False)\n",
    "\n",
    "    print(\"Motion COrrection\")\n",
    "    # %%% MOTION CORRECTION\n",
    "    # first we create a motion correction object with the specified parameters\n",
    "    mc = MotionCorrect(fnames, dview=dview, **opts.get_group('motion'))\n",
    "    mc.motion_correct(save_movie=True)\n",
    "    dview.terminate()\n",
    "\n",
    "    # Save MC file\n",
    "    m_rig = cm.load(mc.mmap_file)\n",
    "    m_rig.save(save_name+'_mc_.tif', to32 = False)\n",
    "    print(inspect.signature(m_rig.save))\n",
    "    plt.imshow(mc.total_template_rig, cmap = 'gray')\n",
    "    #plt.imsave(mc.total_template_rig)   \n",
    "    if 'dview' in locals():\n",
    "        cm.stop_server(dview=dview)\n",
    "\n",
    "\n",
    "if False:\n",
    "    filepath = '101123\\\\CY01\\\\100N36_16x\\\\_6\\\\p1.tif'\n",
    "    save_name = 'p1_mc_'\n",
    "    bulk_MC(filepath,save_name)\n",
    "\n",
    "\n",
    "    filepath = '101123\\\\CY01\\\\100N36_16x\\\\_6\\\\p2.tif'\n",
    "    save_name = 'p2_mc_'\n",
    "    bulk_MC(filepath,save_name)\n",
    "\n",
    "    filepath = '101123\\\\CY01\\\\100N36_16x\\\\_6\\\\p2.tif'\n",
    "    save_name = 'p2_mc_'\n",
    "    bulk_MC(filepath,save_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train 3D-UNet on previous WF recording\n",
    "\n",
    "# 3D U-Net Training, adapted from demo_train_pipeline.ipynb (DeepCAD-RT)\n",
    "datasets_path = 'datasets/WF_500_hip'\n",
    "n_epochs = 5                # number of training epochs\n",
    "GPU = '0'                   # the index of GPU you will use (e.g. '0', '0,1', '0,1,2')\n",
    "train_datasets_size = 12000  # datasets size for training (how many 3D patches)\n",
    "patch_xy = 100              # the width and height of 3D patches\n",
    "patch_t = 1000              # the time dimension (frames) of 3D patches\n",
    "overlap_factor = 0.6        # the overlap factor between two adjacent patches\n",
    "pth_dir = './pth'           # the path for pth file and result images \n",
    "num_workers = 0             # if you use Windows system, set this to 0.\n",
    "\n",
    "# Setup some parameters for result visualization during training period (optional)\n",
    "visualize_images_per_epoch = True  # whether to show result images after each epoch\n",
    "save_test_images_per_epoch = True  # whether to save result images after each epoch\n",
    "\n",
    "\n",
    "train_dict = {\n",
    "    # dataset dependent parameters\n",
    "    'patch_x': patch_xy,                          # the width of 3D patches\n",
    "    'patch_y': patch_xy,                          # the height of 3D patches\n",
    "    'patch_t': patch_t,                           # the time dimension (frames) of 3D patches\n",
    "    'overlap_factor':overlap_factor,              # overlap factor\n",
    "    'scale_factor': 1,                            # the factor for image intensity scaling\n",
    "    'select_img_num': 1000000,                    # select the number of frames used for training (use all frames by default)\n",
    "    'train_datasets_size': train_datasets_size,   # datasets size for training (how many 3D patches)\n",
    "    'datasets_path': datasets_path,               # folder containing files for training\n",
    "    'pth_dir': pth_dir,                           # the path for pth file and result images \n",
    "    \n",
    "    # network related parameters\n",
    "    'n_epochs': n_epochs,                         # the number of training epochs\n",
    "    'lr': 0.00005,                                # learning rate\n",
    "    'b1': 0.5,                                    # Adam: bata1\n",
    "    'b2': 0.999,                                  # Adam: bata2\n",
    "    'fmap': 16,                                   # model complexity\n",
    "    'GPU': GPU,                                   # GPU index\n",
    "    'num_workers': num_workers,                   # if you use Windows system, set this to 0.\n",
    "    'visualize_images_per_epoch': visualize_images_per_epoch,   # whether to show result images after each epoch\n",
    "    'save_test_images_per_epoch': save_test_images_per_epoch    # whether to save result images after each epoch\n",
    "}\n",
    "\n",
    "tc = training_class(train_dict)\n",
    "tc.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Apply pretained model to each phase \n",
    "\n",
    "# 3D U-Net Testing, adapted from demo_test_pipeline.ipynb (DeepCAD-RT)\n",
    "datasets_path = 'datasets/MC100'\n",
    "denoise_model = 'WF_500_hip'\n",
    "\n",
    "test_datasize = 6000         \n",
    "GPU = '0'                            \n",
    "patch_xy = 100                \n",
    "patch_t = 300                    \n",
    "overlap_factor = 0.6                \n",
    "num_workers = 0                      \n",
    "visualize_images_per_epoch = False    \n",
    "save_test_images_per_epoch = True     \n",
    "\n",
    "test_dict = {\n",
    "    # dataset dependent parameters\n",
    "    'patch_x': patch_xy,                 # the width of 3D patches\n",
    "    'patch_y': patch_xy,                 # the height of 3D patches\n",
    "    'patch_t': patch_t,                  # the time dimension (frames) of 3D patches\n",
    "    'overlap_factor':overlap_factor,     # overlap factor, \n",
    "    'scale_factor': 1,                   # the factor for image intensity scaling\n",
    "    'test_datasize': test_datasize,      # the number of frames to be tested\n",
    "    'datasets_path': datasets_path,      # folder containing all files to be tested\n",
    "    'pth_dir': './pth',                  # pth file root path\n",
    "    'denoise_model' : denoise_model,     # A folder containing all models to be tested\n",
    "    'output_dir' : './results',          # result file root path\n",
    "    # network related parameters\n",
    "    'fmap': 16,                          # number of feature maps\n",
    "    'GPU': GPU,                          # GPU index\n",
    "    'num_workers': num_workers,          # if you use Windows system, set this to 0.\n",
    "    'visualize_images_per_epoch': visualize_images_per_epoch,  # whether to display inference performance after each epoch\n",
    "    'save_test_images_per_epoch': save_test_images_per_epoch   # whether to save inference image after each epoch in pth path\n",
    "}\n",
    "\n",
    "tc = testing_class(test_dict)\n",
    "tc.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Re-interleave phase stacks \n",
    "\n",
    "def interleave_stacks(filename_p1,filename_p2, filename_p3):\n",
    "    p1 = tifffile.imread(filename_p1)\n",
    "    p2 = tifffile.imread(filename_p2)\n",
    "    p3 = tifffile.imread(filename_p3)\n",
    "    #p4 = tifffile.imread(filename_p4)\n",
    "    img = np.zeros((p1.shape[0]*3,p1.shape[1],p1.shape[2]),dtype=np.uint16)\n",
    "    img[0::3,:,:] = p1\n",
    "    img[1::3,:,:] = p2\n",
    "    img[2::3,:,:] = p3\n",
    "    #img[3::4,:,:] = p4\n",
    "    tifffile.imsave(filename_p1[:-6]+'_i.tif', img)\n",
    "\n",
    "if False:\n",
    "    filename_p1 = '122123\\\\CY01_16x\\\\CY01_16x\\\\n120_100_3\\\\DeepCAD\\\\FB\\\\p1_best_model_output.tif'\n",
    "    filename_p2 = '122123\\\\CY01_16x\\\\CY01_16x\\\\n120_100_3\\\\DeepCAD\\\\FB\\\\p2_best_model_output.tif'\n",
    "    filename_p3 = '122123\\\\CY01_16x\\\\CY01_16x\\\\n120_100_3\\\\DeepCAD\\\\FB\\\\p3_best_model_output.tif'\n",
    "    interleave_stacks(filename_p1,filename_p2, filename_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Perform interleaved OS-SIM and pWF Reconstructions, adapted from SIMScope3D Reconstruction\n",
    "\n",
    "def optical_sectioning_sim_I(imgs, method, flag):\n",
    "    if flag == 0:\n",
    "        I1 = imgs[0,:]\n",
    "        I2 = imgs[1,:]\n",
    "        I3 = imgs[2,:]\n",
    "    elif flag == 1:\n",
    "        I1 = imgs[2,:]\n",
    "        I2 = imgs[0,:]\n",
    "        I3 = imgs[1,:]\n",
    "    elif flag == 2:\n",
    "        I1 = imgs[1,:]\n",
    "        I2 = imgs[2,:]\n",
    "        I3 = imgs[0,:]\n",
    "\n",
    "    if method == 'DD': # for 90 degree phase shifts\n",
    "        os_image = 0.5 * np.sqrt((2*I2 - I1 - I3)**2 + (I3 - I1)**2) \n",
    "\n",
    "    elif method == 'NEIL': # for 120 degree phase shifts\n",
    "        os_image = np.sqrt(((I1-I2)**2)+((I1-I3)**2)+((I2-I3)**2))\n",
    "\n",
    "    else:\n",
    "        print('Invalid method. Please choose either DD or NEIL')  \n",
    "    \n",
    "    return os_image\n",
    "\n",
    "def match_histogram_z_I(imgs, nangles, nphases, flag):\n",
    "    for ii in range(nangles):\n",
    "        for jj in range(0, nphases):\n",
    "            if jj != flag:\n",
    "               imgs[ii, jj,:] = match_histograms(imgs[ii, jj,:], imgs[ii, flag,:])\n",
    "\n",
    "    return imgs\n",
    "\n",
    "def run_SIM_Interleaved(file_Name, method, match_hist):\n",
    "    dx = 1.0\n",
    "    dz = 1.0\n",
    "    excitation_wl = 0.470\n",
    "    emission_wl = 0.520\n",
    "    na = 0.3\n",
    "    nangles = 1\n",
    "    nphases = 3\n",
    "\n",
    "    # load the data and reshape\n",
    "    input_file_path = Path(file_Name)\n",
    "    #input_file_path = Path(\"c://users//researcher/downloads/TRYSIM_8.tif\")\n",
    "    root_path = input_file_path.parents[0]\n",
    "    img = tifffile.imread(input_file_path)\n",
    "    print(img.shape)\n",
    "    \n",
    "\n",
    "    # TO DO: correctly parse metadata / load multiple images loop over timelapse\n",
    "    nt = 1\n",
    "    nz = int(img.shape[0]/(nangles*nphases))\n",
    "    ny = img.shape[1]\n",
    "    nx = img.shape[2]\n",
    "\n",
    "\n",
    "\n",
    "    img1 = img[0:3*nz,:,:]\n",
    "    img2 = img[1:3*nz-2,:,:]\n",
    "    img3 = img[2:3*nz-1,:,:]\n",
    "\n",
    "    print(img1.shape)\n",
    "\n",
    "    nz = nz\n",
    "\n",
    "\n",
    "    img1_reshape = np.reshape(img1,[nz,nangles,nphases,ny,nx])\n",
    "    img2_reshape = np.reshape(img2,[nz-1,nangles,nphases,ny,nx])\n",
    "    img3_reshape = np.reshape(img3,[nz-1,nangles,nphases,ny,nx])\n",
    "\n",
    "    # turn image into float\n",
    "    #img_reshape = img_reshape.astype(np.float32)\n",
    "    img1_reshape = img1_reshape.astype(np.float32)\n",
    "    img2_reshape = img2_reshape.astype(np.float32)\n",
    "    img3_reshape = img3_reshape.astype(np.float32)\n",
    "\n",
    "    # create storage variables\n",
    "    widefield1 = np.zeros((nz,ny,nx),dtype=np.float32)\n",
    "    os_sim_per_angle1 = np.zeros((nangles,ny,nx),dtype=np.float32)\n",
    "    os_sim_angle1 = np.zeros((nangles,nz,ny,nx),dtype=np.float32)\n",
    "    os_sim1 = np.zeros((nz,ny,nx),dtype=np.float32)\n",
    "\n",
    "      \n",
    "    widefield2 = np.zeros((nz,ny,nx),dtype=np.float32)\n",
    "    os_sim_per_angle2 = np.zeros((nangles,ny,nx),dtype=np.float32)\n",
    "    os_sim_angle2 = np.zeros((nangles,nz,ny,nx),dtype=np.float32)\n",
    "    os_sim2 = np.zeros((nz,ny,nx),dtype=np.float32)\n",
    "\n",
    "    widefield3 = np.zeros((nz,ny,nx),dtype=np.float32)  \n",
    "    os_sim_per_angle3 = np.zeros((nangles,ny,nx),dtype=np.float32)\n",
    "    os_sim_angle3 = np.zeros((nangles,nz,ny,nx),dtype=np.float32)\n",
    "    os_sim3 = np.zeros((nz,ny,nx),dtype=np.float32)\n",
    "\n",
    "\n",
    "    # loop over all timepoints\n",
    "    for t_idx in tqdm(range(0,nt),desc='time',leave=False):\n",
    "\n",
    "        # check if there is more than one time point\n",
    "        if nt==1:\n",
    "            imgs_to_process1 = img1_reshape[:]\n",
    "            imgs_to_process2 = img2_reshape[:]\n",
    "            imgs_to_process3 = img3_reshape[:]\n",
    "        else:\n",
    "            imgs_to_process = img1_reshape[t_idx,:]\n",
    "        \n",
    "        for z_idx in tqdm(range(0,nz-1),desc='SIM OS per z plane',leave=True):\n",
    "            \n",
    "            if match_hist == True:\n",
    "                matched_imgs1 = match_histogram_z_I(imgs_to_process1[z_idx,:],nangles,nphases,0)\n",
    "                matched_imgs2 = match_histogram_z_I(imgs_to_process2[z_idx,:],nangles,nphases,2)\n",
    "                matched_imgs3 = match_histogram_z_I(imgs_to_process3[z_idx,:],nangles,nphases,1)\n",
    "\n",
    "            else:    \n",
    "                matched_imgs1 = imgs_to_process1[z_idx,:]\n",
    "                matched_imgs2 = imgs_to_process2[z_idx,:]\n",
    "                matched_imgs3 = imgs_to_process3[z_idx,:]\n",
    "\n",
    "            # calculate widefield image at this z plane\n",
    "            widefield1[z_idx,:] = np.nanmean(matched_imgs1, axis=(0, 1))\n",
    "            widefield2[z_idx,:] = np.nanmean(matched_imgs2, axis=(0, 1))\n",
    "            widefield3[z_idx,:] = np.nanmean(matched_imgs3, axis=(0, 1))\n",
    "            #print(matched_imgs.shape)\n",
    "            \"\"\"\n",
    "            if method == 'DD':\n",
    "                matched_imgs_DD = matched_imgs[:,::2,:,:]\n",
    "                widefield[z_idx,:] = np.nanmean(matched_imgs_DD,axis=(0, 1))\n",
    "            \"\"\"\n",
    "            \n",
    "            # calculate os-sim image for each angle at this z plane\n",
    "            for angle_idx in range(0,nangles):\n",
    "                os_sim_per_angle1[angle_idx,:]=optical_sectioning_sim_I(matched_imgs1[angle_idx,:],method,0)\n",
    "                os_sim_angle1[angle_idx,z_idx,:]=os_sim_per_angle1[angle_idx,:]\n",
    "\n",
    "                os_sim_per_angle2[angle_idx,:]=optical_sectioning_sim_I(matched_imgs2[angle_idx,:],method,1)\n",
    "                os_sim_angle2[angle_idx,z_idx,:]=os_sim_per_angle2[angle_idx,:]\n",
    "\n",
    "                os_sim_per_angle3[angle_idx,:]=optical_sectioning_sim_I(matched_imgs3[angle_idx,:],method,2)\n",
    "                os_sim_angle3[angle_idx,z_idx,:]=os_sim_per_angle3[angle_idx,:]\n",
    "\n",
    "            # average os-sim over all angles at this z plane\n",
    "            os_sim1[z_idx,:] = np.nanmean(os_sim_per_angle1,axis=0)\n",
    "            os_sim2[z_idx,:] = np.nanmean(os_sim_per_angle2,axis=0)\n",
    "            os_sim3[z_idx,:] = np.nanmean(os_sim_per_angle3,axis=0)\n",
    "\n",
    "    os_img = np.zeros((3*nz,ny,nx),dtype=np.float32)\n",
    "    os_img[0::3,:,:] = os_sim1\n",
    "    os_img[1::3,:,:] = os_sim2\n",
    "    os_img[2::3,:,:] = os_sim3 \n",
    "\n",
    "    wf_img = np.zeros((3*nz,ny,nx),dtype=np.float32)\n",
    "    wf_img[0::3,:,:] = widefield1\n",
    "    wf_img[1::3,:,:] = widefield2\n",
    "    wf_img[2::3,:,:] = widefield3 \n",
    "    \n",
    "    os_img = rescale_intensity(os_img,out_range=(0,65535)).astype(np.uint16)\n",
    "    wf_img = rescale_intensity(wf_img,out_range=(0,65535)).astype(np.uint16)\n",
    "\n",
    "    return  os_img, wf_img\n",
    "\n",
    "def save_Reconstructions_i(wf_img, os_img, input_file, match_hist):\n",
    "    # remove last 6 frames before saving\n",
    "    wf_img = wf_img[:-6,:,:]\n",
    "    os_img = os_img[:-6,:,:]\n",
    "    if match_hist == True:\n",
    "        output_file_path_SIM = Path(input_file.rsplit('.', 1)[0] + '_interleaved_matchSIM_Reconstruction.tif')\n",
    "        output_file_path_pWF = Path(input_file.rsplit('.', 1)[0] + '_interleaved_matchpWF_Reconstruction.tif')\n",
    "        #output_file_path_MCN = Path(input_file.rsplit('.', 1)[0] + '_MCNR_Reconstruction.tif')\n",
    "    else:\n",
    "\n",
    "        output_file_path_SIM = Path(input_file.rsplit('.', 1)[0] + '_interleaved_NOmatchSIM_Reconstruction.tif')\n",
    "        output_file_path_pWF = Path(input_file.rsplit('.', 1)[0] + '_interleaved_NOmatchpWF_Reconstruction.tif')\n",
    "        \n",
    "    tifffile.imwrite(output_file_path_SIM,os_img)\n",
    "    tifffile.imwrite(output_file_path_pWF,wf_img)\n",
    "\n",
    "def make_numFrames_divisible_by_3(fileName):\n",
    "    img = tifffile.imread(fileName)\n",
    "    img = img[0:img.shape[0]-img.shape[0]%3,:,:]\n",
    "    tifffile.imsave(fileName[:-4]+'_divisible.tif', img)\n",
    "\n",
    "if False:\n",
    "    folder_Name = 'GEVI_SIM'\n",
    "    filename = 'N36_400_X.tif'\n",
    "    method = 'NEIL'\n",
    "    match_hist = False\n",
    "    try:\n",
    "        os_img, wf_img = run_SIM_Interleaved(folder_Name + '\\\\' + filename, method, match_hist)\n",
    "        save_Reconstructions_i(wf_img, os_img,  folder_Name +'\\\\' + method+ '_1_' + filename, match_hist)\n",
    "\n",
    "    except:\n",
    "        make_numFrames_divisible_by_3(folder_Name + '\\\\' + folder_N + '\\\\'+ filename)\n",
    "        os_img, wf_img = run_SIM_Interleaved(folder_Name + '\\\\' + filename[:-4]+'_divisible.tif', method, match_hist)\n",
    "        save_Reconstructions_i(wf_img, os_img,  folder_Name +'\\\\' + method+ '_1_' + filename[:-4]+'_divisible.tif', match_hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 (applied to reconstructions that were not preprocessed)\n",
    "\n",
    "# Function for seeded motion correction, adapted from VolPy\n",
    "def SEED_MC(filepath, filepath2,save_name):\n",
    "    print(\"bulk\")\n",
    "    \n",
    "    fnames = os.fsdecode(filepath)\n",
    "    fnames2 = os.fsdecode(filepath2)\n",
    "    file_dir = os.path.split(fnames)[0]\n",
    "    \n",
    "  \n",
    "    fr = 500                                     \n",
    "    ROIs = None                                    \n",
    "    index = None                                   \n",
    "    weights = None                                \n",
    "                                                   \n",
    "\n",
    "    pw_rigid = False                                \n",
    "    gSig_filt = (6, 6)                             \n",
    "                                                   \n",
    "    max_shifts = (10, 10)                            \n",
    "    strides = (20, 20)                            \n",
    "    overlaps = (24, 24)                            \n",
    "    max_deviation_rigid = 3                         \n",
    "    border_nan = 'copy'\n",
    "\n",
    "    opts_dict = {\n",
    "        'fnames': fnames,\n",
    "        'fr': fr,\n",
    "        'index': index,\n",
    "        'ROIs': ROIs,\n",
    "        'weights': weights,\n",
    "        'pw_rigid': pw_rigid,\n",
    "        'max_shifts': max_shifts,\n",
    "        'gSig_filt': gSig_filt,\n",
    "        'strides': strides,\n",
    "        'overlaps': overlaps,\n",
    "        'max_deviation_rigid': max_deviation_rigid,\n",
    "        'border_nan': border_nan\n",
    "    }\n",
    "\n",
    "    opts = volparams(params_dict=opts_dict)\n",
    "\n",
    "    if 'dview' in locals():\n",
    "        cm.stop_server(dview=dview)\n",
    "\n",
    "    c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "            backend='local', n_processes=50, single_thread=False)\n",
    "\n",
    "    print(\"Motion COrrection\")\n",
    "    # %%% MOTION CORRECTION\n",
    "    # first we create a motion correction object with the specified parameters\n",
    "    mc = MotionCorrect(fnames, dview=dview, **opts.get_group('motion'))\n",
    "    mc.motion_correct(save_movie=True)\n",
    "    dview.terminate()\n",
    "\n",
    "    # Save MC file\n",
    "    m_rig = cm.load(mc.mmap_file)\n",
    "    m_rig.save(save_name+'_mc_.tif', to32 = False)\n",
    "\n",
    "\n",
    "    mmap_file = mc.apply_shifts_movie(fnames2, save_memmap=True, order='C')\n",
    "    m_rig = cm.load(mmap_file)\n",
    "    m_rig.save(save_name + '_seed_mc_.tif', to32 = False)\n",
    "\n",
    "\n",
    "    print(inspect.signature(m_rig.save))\n",
    "    plt.imshow(mc.total_template_rig, cmap = 'gray')\n",
    "    #plt.imsave(mc.total_template_rig)   \n",
    "\n",
    "if False:\n",
    "    filepath = '122123\\\\CY01_16x\\\\CY01_16x\\\\n90_500_b4_1\\\\NEIL_1_n90_500_b4_MMStack.ome_interleaved_NOmatchpWF_Reconstruction.tif'\n",
    "    filepath2 = '122123\\\\CY01_16x\\\\CY01_16x\\\\n90_500_b4_1\\\\DeepCAD\\\\N2N_WF500\\\\NEIL_1_p1_E_05_Iter_6254_outp_i_interleaved_NOmatchpWF_Reconstruction.tif'\n",
    "    save_name = 'Seeded_WF_n2n'\n",
    "    SEED_MC(filepath,filepath2,save_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
